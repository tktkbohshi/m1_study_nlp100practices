{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "section09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tktkbohshi/m1_study_nlp100practices/blob/main/section09/section09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxqBHybRYzq8"
      },
      "source": [
        "# 第9章: RNN, CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytC4dDb9DWP8"
      },
      "source": [
        "#データの読込　マウントした方がよいのか。\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5oD-TlFZCKV"
      },
      "source": [
        "## 80. ID番号への変換\n",
        "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq9OyQziDpBR",
        "outputId": "207f81ce-3262-4e4d-f4a9-f6d9f0d72d9b"
      },
      "source": [
        "# データのダウンロード\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip NewsAggregatorDataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-21 10:43:52--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29224203 (28M) [application/x-httpd-php]\n",
            "Saving to: ‘NewsAggregatorDataset.zip.1’\n",
            "\n",
            "NewsAggregatorDatas 100%[===================>]  27.87M  31.1MB/s    in 0.9s    \n",
            "\n",
            "2021-09-21 10:43:54 (31.1 MB/s) - ‘NewsAggregatorDataset.zip.1’ saved [29224203/29224203]\n",
            "\n",
            "Archive:  NewsAggregatorDataset.zip\n",
            "replace 2pageSessions.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: 2pageSessions.csv       \n",
            "replace __MACOSX/._2pageSessions.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/._2pageSessions.csv  \n",
            "replace newsCorpora.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: newsCorpora.csv         \n",
            "  inflating: __MACOSX/._newsCorpora.csv  \n",
            "  inflating: readme.txt              \n",
            "  inflating: __MACOSX/._readme.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXsfL2mW9U3L"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KiTWx26Dpl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecccd7b-bf1d-461f-f32c-c9359bad56d9"
      },
      "source": [
        "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
        "!sed -e 's/\"/'\\''/g' ./newsCorpora.csv > ./newsCorpora_re.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeyUHH3jY9vx",
        "outputId": "a5bcc42f-ac07-4ae3-d996-6f392d214fb3"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データの読込\n",
        "df = pd.read_csv('./newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "\n",
        "# データの抽出\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "# データの分割\n",
        "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
        "\n",
        "# 事例数の確認\n",
        "print('【学習データ】')\n",
        "print(train['CATEGORY'].value_counts())\n",
        "print('【検証データ】')\n",
        "print(valid['CATEGORY'].value_counts())\n",
        "print('【評価データ】')\n",
        "print(test['CATEGORY'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "【学習データ】\n",
            "b    4501\n",
            "e    4235\n",
            "t    1220\n",
            "m     728\n",
            "Name: CATEGORY, dtype: int64\n",
            "【検証データ】\n",
            "b    563\n",
            "e    529\n",
            "t    153\n",
            "m     91\n",
            "Name: CATEGORY, dtype: int64\n",
            "【評価データ】\n",
            "b    563\n",
            "e    530\n",
            "t    152\n",
            "m     91\n",
            "Name: CATEGORY, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wBBD6hbQ64W",
        "outputId": "da0eb556-f41b-4535-e73c-db6d2178095c"
      },
      "source": [
        "from collections import defaultdict\n",
        "import string\n",
        "\n",
        "# 単語の頻度集計\n",
        "d = defaultdict(int)\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))  # 記号をスペースに置換するテーブル\n",
        "for text in train['TITLE']:\n",
        "  for word in text.translate(table).split():\n",
        "    d[word] += 1\n",
        "d = sorted(d.items(), key=lambda x:x[1], reverse=True)\n",
        "\n",
        "# 単語ID辞書の作成\n",
        "word2id = {word: i + 1 for i, (word, cnt) in enumerate(d) if cnt > 1}  # 出現頻度が2回以上の単語を登録\n",
        "\n",
        "print(f'ID数: {len(set(word2id.values()))}\\n')\n",
        "print('---頻度上位20語---')\n",
        "for key in list(word2id)[:20]:\n",
        "    print(f'{key}: {word2id[key]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID数: 9405\n",
            "\n",
            "---頻度上位20語---\n",
            "to: 1\n",
            "s: 2\n",
            "in: 3\n",
            "on: 4\n",
            "UPDATE: 5\n",
            "as: 6\n",
            "US: 7\n",
            "for: 8\n",
            "The: 9\n",
            "of: 10\n",
            "1: 11\n",
            "To: 12\n",
            "2: 13\n",
            "the: 14\n",
            "and: 15\n",
            "In: 16\n",
            "Of: 17\n",
            "a: 18\n",
            "at: 19\n",
            "A: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDpyZL6qQ8L7"
      },
      "source": [
        "def tokenizer(text, word2id=word2id, unk=0):\n",
        "  \"\"\" 入力テキストをスペースで分割しID列に変換(辞書になければunkで指定した数字を設定)\"\"\"\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  return [word2id.get(word, unk) for word in text.translate(table).split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwalZOs1RAu2",
        "outputId": "fc7dbdfb-149a-412f-c3c2-2358cb3befa8"
      },
      "source": [
        "# 確認\n",
        "text = train.iloc[1, train.columns.get_loc('TITLE')]\n",
        "print(f'テキスト: {text}')\n",
        "print(f'ID列: {tokenizer(text)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "テキスト: Amazon Plans to Fight FTC Over Mobile-App Purchases\n",
            "ID列: [169, 539, 1, 683, 1237, 82, 279, 1898, 4199]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqMI4NUwPmA1"
      },
      "source": [
        "## 81. RNNによる予測\n",
        "D番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリyを予測するモデルとして，次式を実装せよ．\n",
        "\n",
        "h→0=0,h→t=RNN−→−−(emb(xt),h→t−1),y=softmax(W(yh)h→T+b(y))\n",
        "ただし，emb(x)∈ℝdwは単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），h→t∈ℝdhは時刻tの隠れ状態ベクトル，RNN−→−−(x,h)は入力xと前時刻の隠れ状態hから次状態を計算するRNNユニット，W(yh)∈ℝL×dhは隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈ℝLはバイアス項である（dw,dh,Lはそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニットRNN−→−−(x,h)には様々な構成が考えられるが，典型例として次式が挙げられる．\n",
        "\n",
        "RNN−→−−(x,h)=g(W(hx)x+W(hh)h+b(h))\n",
        "ただし，W(hx)∈ℝdh×dw，W(hh)∈ℝdh×dh,b(h)∈ℝdhはRNNユニットのパラメータ，gは活性化関数（例えばtanhやReLUなど）である．\n",
        "\n",
        "なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでyを計算するだけでよい．次元数などのハイパーパラメータは，dw=300,dh=50など，適当な値に設定せよ（以降の問題でも同様である）．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt9ro2HRP0d0"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "    self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.batch_size = x.size()[0]\n",
        "    hidden = self.init_hidden()  # h0のゼロベクトルを作成\n",
        "    emb = self.emb(x)\n",
        "    # emb.size() = (batch_size, seq_len, emb_size)\n",
        "    out, hidden = self.rnn(emb, hidden)\n",
        "    # out.size() = (batch_size, seq_len, hidden_size)\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    # out.size() = (batch_size, output_size)\n",
        "    return out\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden = torch.zeros(1, self.batch_size, self.hidden_size)\n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDYKKixBP5qa"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CreateDataset(Dataset):\n",
        "  def __init__(self, X, y, tokenizer):\n",
        "    #print(f\"X:{X}\")\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):  # len(Dataset)で返す値を指定\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
        "  #pandasのtext = x[index]がうまくいかなかったため、pandasを一度リスト化してインデックスを返せるように変更。処理は重くなる。\n",
        "    x = self.X.values.tolist()\n",
        "    text = x[index]\n",
        "    inputs = self.tokenizer(text)\n",
        "\n",
        "    return {\n",
        "      'inputs': torch.tensor(inputs, dtype=torch.int64),\n",
        "      'labels': torch.tensor(self.y[index], dtype=torch.int64)\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KGEFCIAP_1j",
        "outputId": "9400717d-841e-46d2-db3e-7e0f0768cd0f"
      },
      "source": [
        "# ラベルベクトルの作成\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "y_train = train['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "y_valid = valid['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "y_test = test['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "\n",
        "# Datasetの作成\n",
        "dataset_train = CreateDataset(train['TITLE'], y_train, tokenizer)\n",
        "dataset_valid = CreateDataset(valid['TITLE'], y_valid, tokenizer)\n",
        "dataset_test = CreateDataset(test['TITLE'], y_test, tokenizer)\n",
        "\n",
        "print(f'len(Dataset)の出力: {len(dataset_train)}')\n",
        "print(f\"Dataset[index]の出力:{dataset_train[1]}\")\n",
        "#for var in dataset_train[1]:\n",
        "#  print(f'  {var}: {dataset_train[1][var]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(Dataset)の出力: 10684\n",
            "Dataset[index]の出力:{'inputs': tensor([ 169,  539,    1,  683, 1237,   82,  279, 1898, 4199]), 'labels': tensor(1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suHWMQ6CQCV9",
        "outputId": "e5201f18-9c6f-4b4c-893e-ce35ce9c3a77"
      },
      "source": [
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1  # 辞書のID数 + パディングID\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# 先頭10件の予測値取得\n",
        "for i in range(10):\n",
        "  X = dataset_train[i]['inputs']\n",
        "  print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4471, 0.1659, 0.2728, 0.1142]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.1594, 0.1449, 0.3168, 0.3790]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.3105, 0.1754, 0.3148, 0.1993]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.1729, 0.2858, 0.3174, 0.2239]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.1349, 0.3400, 0.3511, 0.1740]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.2154, 0.2354, 0.1290, 0.4202]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.3329, 0.1505, 0.3183, 0.1983]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.1844, 0.4069, 0.1895, 0.2192]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.2360, 0.2308, 0.2775, 0.2557]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.0999, 0.4677, 0.2389, 0.1935]], grad_fn=<SoftmaxBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK3sMCTOcp9D"
      },
      "source": [
        "## 82. 確率的勾配降下法による学習\n",
        "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBZ7QV_ic2zs"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torch import optim\n",
        "\n",
        "def calculate_loss_and_accuracy(model, dataset, device=None, criterion=None):\n",
        "  \"\"\"損失・正解率を計算\"\"\"\n",
        "  dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "  loss = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in dataloader:\n",
        "      # デバイスの指定\n",
        "      inputs = data['inputs'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "\n",
        "      # 順伝播\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # 損失計算\n",
        "      if criterion != None:\n",
        "        loss += criterion(outputs, labels).item()\n",
        "\n",
        "      # 正解率計算\n",
        "      pred = torch.argmax(outputs, dim=-1)\n",
        "      total += len(inputs)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return loss / len(dataset), correct / total\n",
        "\n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, collate_fn=None, device=None):\n",
        "  \"\"\"モデルの学習を実行し、損失・正解率のログを返す\"\"\"\n",
        "  # デバイスの指定\n",
        "  model.to(device)\n",
        "\n",
        "  # dataloaderの作成\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
        "\n",
        "  # スケジューラの設定\n",
        "  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min=1e-5, last_epoch=-1)\n",
        "\n",
        "  # 学習\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(num_epochs):\n",
        "    # 開始時刻の記録\n",
        "    s_time = time.time()\n",
        "\n",
        "    # 訓練モードに設定\n",
        "    model.train()\n",
        "    for data in dataloader_train:\n",
        "      # 勾配をゼロで初期化\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "      inputs = data['inputs'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "      outputs = model(inputs).to(device)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # 評価モードに設定\n",
        "    model.eval()\n",
        "\n",
        "    # 損失と正解率の算出\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataset_train, device, criterion=criterion)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataset_valid, device, criterion=criterion)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    # チェックポイントの保存\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    # 終了時刻の記録\n",
        "    e_time = time.time()\n",
        "\n",
        "    # ログを出力\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "    # 検証データの損失が3エポック連続で低下しなかった場合は学習終了\n",
        "    if epoch > 2 and log_valid[epoch - 3][0] <= log_valid[epoch - 2][0] <= log_valid[epoch - 1][0] <= log_valid[epoch][0]:\n",
        "      break\n",
        "\n",
        "    # スケジューラを1ステップ進める\n",
        "    scheduler.step()\n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArO02qcYc-tS"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def visualize_logs(log):\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "  ax[0].plot(np.array(log['train']).T[0], label='train')\n",
        "  ax[0].plot(np.array(log['valid']).T[0], label='valid')\n",
        "  ax[0].set_xlabel('epoch')\n",
        "  ax[0].set_ylabel('loss')\n",
        "  ax[0].legend()\n",
        "  ax[1].plot(np.array(log['train']).T[1], label='train')\n",
        "  ax[1].plot(np.array(log['valid']).T[1], label='valid')\n",
        "  ax[1].set_xlabel('epoch')\n",
        "  ax[1].set_ylabel('accuracy')\n",
        "  ax[1].legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iGBHlEzdAIn",
        "outputId": "9f713be4-5192-4bf5-ef13-68953198eb71"
      },
      "source": [
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1 \n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, loss_train: 1.0950, accuracy_train: 0.5325, loss_valid: 1.1175, accuracy_valid: 0.5352, 91.2108sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dVRG71UdKbh"
      },
      "source": [
        "# ログの可視化\n",
        "visualize_logs(log)\n",
        "\n",
        "# 正解率の算出\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YEVeeN6kMAF"
      },
      "source": [
        "## 83. ミニバッチ化・GPU上での学習\n",
        "問題82のコードを改変し，BB事例ごとに損失・勾配を計算して学習を行えるようにせよ（BBの値は適当に選べ）．また，GPU上で学習を実行せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf6pRRVVkTrB"
      },
      "source": [
        "class Padsequence():\n",
        "  \"\"\"Dataloaderからミニバッチを取り出すごとに最大系列長でパディング\"\"\"\n",
        "  def __init__(self, padding_idx):\n",
        "    self.padding_idx = padding_idx\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    sorted_batch = sorted(batch, key=lambda x: x['inputs'].shape[0], reverse=True)\n",
        "    sequences = [x['inputs'] for x in sorted_batch]\n",
        "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)\n",
        "    labels = torch.LongTensor([x['labels'] for x in sorted_batch])\n",
        "\n",
        "    return {'inputs': sequences_padded, 'labels': labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlwSvOMpkVfA"
      },
      "source": [
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "\n",
        "# デバイスの指定　--colabの上限いってしまったためストップ\n",
        "device = torch.device('cuda')\n",
        "print(device)\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE).to(device)\n",
        "#RuntimeError: Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cp のため追加\n",
        "model = model.to(device)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMIgLv2WkcW0"
      },
      "source": [
        "# ログの可視化\n",
        "visualize_logs(log)\n",
        "\n",
        "# 正解率の算出\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAq1hXyadgj9"
      },
      "source": [
        "## 84. 単語ベクトルの導入\n",
        "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWw5cWzKdsNF"
      },
      "source": [
        "# 学習済み単語ベクトルのダウンロード\n",
        "FILE_ID = \"0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
        "FILE_NAME = \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$FILE_ID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$FILE_ID\" -O $FILE_NAME && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMsaIzyTd8J4"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# 学習済みモデルのロード\n",
        "model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "# 学習済み単語ベクトルの取得\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "weights = np.zeros((VOCAB_SIZE, EMB_SIZE))\n",
        "words_in_pretrained = 0\n",
        "for i, word in enumerate(word2id.keys()):\n",
        "  try:\n",
        "    weights[i] = model[word]\n",
        "    words_in_pretrained += 1\n",
        "  except KeyError:\n",
        "    weights[i] = np.random.normal(scale=0.4, size=(EMB_SIZE,))\n",
        "weights = torch.from_numpy(weights.astype((np.float32)))\n",
        "\n",
        "print(f'学習済みベクトル利用単語数: {words_in_pretrained} / {VOCAB_SIZE}')\n",
        "print(weights.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbsrpWHud_0E"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size, num_layers, emb_weights=None, bidirectional=False):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.num_directions = bidirectional + 1  # 単方向：1、双方向：2\n",
        "    if emb_weights != None:  # 指定があれば埋め込み層の重みをemb_weightsで初期化\n",
        "      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "    else:\n",
        "      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "    self.rnn = nn.RNN(emb_size, hidden_size, num_layers, nonlinearity='tanh', bidirectional=bidirectional, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.batch_size = x.size()[0]\n",
        "    hidden = self.init_hidden()  # h0のゼロベクトルを作成\n",
        "    emb = self.emb(x)\n",
        "    # emb.size() = (batch_size, seq_len, emb_size)\n",
        "    out, hidden = self.rnn(emb, hidden)\n",
        "    # out.size() = (batch_size, seq_len, hidden_size * num_directions)\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    # out.size() = (batch_size, output_size)\n",
        "    return out\n",
        "\n",
        "  def init_hidden(self):\n",
        "    hidden = torch.zeros(self.num_layers * self.num_directions, self.batch_size, self.hidden_size)\n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdxjy4yjeCz6"
      },
      "source": [
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "NUM_LAYERS = 1\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, emb_weights=weights)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# デバイスの指定\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcZW_K_MhLIc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfUdVZ8TeHNF"
      },
      "source": [
        "# ログの可視化\n",
        "visualize_logs(log)\n",
        "\n",
        "# 正解率の算出\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "banFPnCNeWoX"
      },
      "source": [
        "## 85. 双方向RNN・多層化\n",
        "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n",
        "\n",
        "h←T+1=0,h←t=RNN←−−−(emb(xt),h←t+1),y=softmax(W(yh)[h⃗ T;h←1]+b(y))\n",
        "h←T+1=0,h←t=RNN←(emb(xt),h←t+1),y=softmax(W(yh)[h→T;h←1]+b(y))\n",
        "ただし，h⃗ t∈ℝdh,h←t∈ℝdhh→t∈Rdh,h←t∈Rdhはそれぞれ，順方向および逆方向のRNNで求めた時刻ttの隠れ状態ベクトル，RNN←−−−(x,h)RNN←(x,h)は入力xxと次時刻の隠れ状態hhから前状態を計算するRNNユニット，W(yh)∈ℝL×2dhW(yh)∈RL×2dhは隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈ℝLb(y)∈RLはバイアス項である．また，[a;b][a;b]はベクトルaaとbbの連結を表す。\n",
        "\n",
        "さらに，双方向RNNを多層化して実験せよ．\n",
        "\n",
        "双方向を指定する引数であるbidirectionalをTrueとし、またNUM_LAYERSを2に設定して学習を実行します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGJPklLZeeVh"
      },
      "source": [
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "NUM_LAYERS = 2\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, emb_weights=weights, bidirectional=True)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# デバイスの指定\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhlxWxljehsn"
      },
      "source": [
        "# ログの可視化\n",
        "visualize_logs(log)\n",
        "\n",
        "# 正解率の算出\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQIt1hMzfRFS"
      },
      "source": [
        "## 86. 畳み込みニューラルネットワーク (CNN)\n",
        "D番号で表現された単語列x=(x1,x2,…,xT)x=(x1,x2,…,xT)がある．ただし，TTは単語列の長さ，xt∈ℝVxt∈RVは単語のID番号のone-hot表記である（VVは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xxからカテゴリyyを予測するモデルを実装せよ．\n",
        "\n",
        "ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n",
        "\n",
        "単語埋め込みの次元数: dwdw\n",
        "畳み込みのフィルターのサイズ: 3 トークン\n",
        "畳み込みのストライド: 1 トークン\n",
        "畳み込みのパディング: あり\n",
        "畳み込み演算後の各時刻のベクトルの次元数: dhdh\n",
        "畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文をdhdh次元の隠れベクトルで表現 すなわち，時刻ttの特徴ベクトルpt∈ℝdhpt∈Rdhは次式で表される．\n",
        "pt=g(W(px)[emb(xt−1);emb(xt);emb(xt+1)]+b(p))$]\n",
        "pt=g(W(px)[emb(xt−1);emb(xt);emb(xt+1)]+b(p))$]\n",
        "ただし，W(px)∈ℝdh×3dw,b(p)∈ℝdhW(px)∈Rdh×3dw,b(p)∈RdhはCNNのパラメータ，ggは活性化関数（例えばtanhtanhやReLUなど），[a;b;c][a;b;c]はベクトルa,b,ca,b,cの連結である．なお，行列W(px)W(px)の列数が3dw3dwになるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n",
        "最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトルc∈ℝdhc∈Rdhを求める．c[i]c[i]でベクトルccのii番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n",
        "\n",
        "c[i]=max1≤t≤Tpt[i]\n",
        "c[i]=max1≤t≤Tpt[i]\n",
        "最後に，入力文書の特徴ベクトルccに行列W(yc)∈ℝL×dhW(yc)∈RL×dhとバイアス項b(y)∈ℝLb(y)∈RLによる線形変換とソフトマックス関数を適用し，カテゴリyyを予測する．\n",
        "\n",
        "y=softmax(W(yc)c+b(y))\n",
        "y=softmax(W(yc)c+b(y))\n",
        "なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列でyyを計算するだけでよい．\n",
        "\n",
        "指定のネットワークを実装します。\n",
        "埋め込み層に続き、nn.Conv2dで畳み込みを計算します。max_poolで系列長方向に最大値を取得しており、この部分で文単位にベクトルが集約されています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5LrvGyqfaje"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, kernel_heights, stride, padding, emb_weights=None):\n",
        "    super().__init__()\n",
        "    if emb_weights != None:  # 指定があれば埋め込み層の重みをemb_weightsで初期化\n",
        "      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "    else:\n",
        "      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "    self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding, 0))\n",
        "    self.drop = nn.Dropout(0.3)\n",
        "    self.fc = nn.Linear(out_channels, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x.size() = (batch_size, seq_len)\n",
        "    emb = self.emb(x).unsqueeze(1)\n",
        "    # emb.size() = (batch_size, 1, seq_len, emb_size)\n",
        "    conv = self.conv(emb)\n",
        "    # conv.size() = (batch_size, out_channels, seq_len, 1)\n",
        "    act = F.relu(conv.squeeze(3))\n",
        "    # act.size() = (batch_size, out_channels, seq_len)\n",
        "    max_pool = F.max_pool1d(act, act.size()[2])\n",
        "    # max_pool.size() = (batch_size, out_channels, 1) -> seq_len方向に最大値を取得\n",
        "    out = self.fc(self.drop(max_pool.squeeze(2)))\n",
        "    # out.size() = (batch_size, output_size)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtlUcPl3ffeV"
      },
      "source": [
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "OUT_CHANNELS = 100\n",
        "KERNEL_HEIGHTS = 3\n",
        "STRIDE = 1\n",
        "PADDING = 1\n",
        "\n",
        "# モデルの定義\n",
        "model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n",
        "\n",
        "# 先頭10件の予測値取得\n",
        "for i in range(10):\n",
        "  X = dataset_train[i]['inputs']\n",
        "  print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoSWR1tUfli0"
      },
      "source": [
        "## 87. 確率的勾配降下法によるCNNの学習\n",
        "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dJTfAdgfuoQ"
      },
      "source": [
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "OUT_CHANNELS = 100\n",
        "KERNEL_HEIGHTS = 3\n",
        "STRIDE = 1\n",
        "PADDING = 1\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# モデルの定義\n",
        "model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# デバイスの指定\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unm15AQHf28t"
      },
      "source": [
        "# ログの可視化\n",
        "visualize_logs(log)\n",
        "\n",
        "# 正解率の算出\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iQmVI0jf0Uy"
      },
      "source": [
        "## 88. パラメータチューニング\n",
        "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAK68zBVgoFD"
      },
      "source": [
        "## 88. パラメータチューニング\n",
        "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz9Jl_8LgEcR"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class textCNN(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, conv_params, drop_rate, emb_weights=None):\n",
        "    super().__init__()\n",
        "    if emb_weights != None:  # 指定があれば埋め込み層の重みをemb_weightsで初期化\n",
        "      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "    else:\n",
        "      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "    self.convs = nn.ModuleList([nn.Conv2d(1, out_channels, (kernel_height, emb_size), padding=(padding, 0)) for kernel_height, padding in conv_params])\n",
        "    self.drop = nn.Dropout(drop_rate)\n",
        "    self.fc = nn.Linear(len(conv_params) * out_channels, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x.size() = (batch_size, seq_len)\n",
        "    emb = self.emb(x).unsqueeze(1)\n",
        "    # emb.size() = (batch_size, 1, seq_len, emb_size)\n",
        "    conv = [F.relu(conv(emb)).squeeze(3) for i, conv in enumerate(self.convs)]\n",
        "    # conv[i].size() = (batch_size, out_channels, seq_len + padding * 2 - kernel_height + 1)\n",
        "    max_pool = [F.max_pool1d(i, i.size(2)) for i in conv]\n",
        "    # max_pool[i].size() = (batch_size, out_channels, 1) -> seq_len方向に最大値を取得\n",
        "    max_pool_cat = torch.cat(max_pool, 1)\n",
        "    # max_pool_cat.size() = (batch_size, len(conv_params) * out_channels, 1)  -> フィルター別の結果を結合\n",
        "    out = self.fc(self.drop(max_pool_cat.squeeze(2)))\n",
        "    # out.size() = (batch_size, output_size)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-bZFo4agHOV"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBDmQI8ggJLR"
      },
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "  # チューニング対象パラメータのセット\n",
        "  emb_size = int(trial.suggest_discrete_uniform('emb_size', 100, 400, 100))\n",
        "  out_channels = int(trial.suggest_discrete_uniform('out_channels', 50, 200, 50))\n",
        "  drop_rate = trial.suggest_discrete_uniform('drop_rate', 0.0, 0.5, 0.1)\n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 5e-4, 5e-2)\n",
        "  momentum = trial.suggest_discrete_uniform('momentum', 0.5, 0.9, 0.1)\n",
        "  batch_size = int(trial.suggest_discrete_uniform('batch_size', 16, 128, 16))\n",
        "\n",
        "  # 固定パラメータの設定\n",
        "  VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "  PADDING_IDX = len(set(word2id.values()))\n",
        "  OUTPUT_SIZE = 4\n",
        "  CONV_PARAMS = [[2, 0], [3, 1], [4, 2]]\n",
        "  NUM_EPOCHS = 30\n",
        "\n",
        "  # モデルの定義\n",
        "  model = textCNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, out_channels, CONV_PARAMS, drop_rate, emb_weights=weights)\n",
        "\n",
        "  # 損失関数の定義\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # オプティマイザの定義\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "  # デバイスの指定\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "  # モデルの学習\n",
        "  log = train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)\n",
        "\n",
        "  # 損失の算出\n",
        "  loss_valid, _ = calculate_loss_and_accuracy(model, dataset_valid, device, criterion=criterion) \n",
        "\n",
        "  return loss_valid "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE5WxTapgNhs"
      },
      "source": [
        "# 最適化\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, timeout=7200)\n",
        "\n",
        "# 結果の表示\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: {:.3f}'.format(trial.value))\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "  print('    {}: {}'.format(key, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6gakAHIgSj2"
      },
      "source": [
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 1\n",
        "EMB_SIZE = int(trial.params['emb_size'])\n",
        "PADDING_IDX = len(set(word2id.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "OUT_CHANNELS = int(trial.params['out_channels'])\n",
        "CONV_PARAMS = [[2, 0], [3, 1], [4, 2]]\n",
        "DROP_RATE = trial.params['drop_rate']\n",
        "LEARNING_RATE = trial.params['learning_rate']\n",
        "BATCH_SIZE = int(trial.params['batch_size'])\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "# モデルの定義\n",
        "model = textCNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, CONV_PARAMS, DROP_RATE, emb_weights=weights)\n",
        "print(model)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "# デバイスの指定\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNTHjgTlgVFt"
      },
      "source": [
        "# ログの可視化\n",
        "visualize_logs(log)\n",
        "\n",
        "# 正解率の算出\n",
        "_, acc_train = calculate_loss_and_accuracy(model, dataset_train, device)\n",
        "_, acc_test = calculate_loss_and_accuracy(model, dataset_test, device)\n",
        "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
        "print(f'正解率（評価データ）：{acc_test:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDkBGvsmgpuL"
      },
      "source": [
        "## 89. 事前学習済み言語モデルからの転移学習\n",
        "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU2BoPkAg7XI"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip NewsAggregatorDataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtvQvtGQjFx2"
      },
      "source": [
        "# 行数の確認\n",
        "!wc -l ./newsCorpora.csvy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ9TlYDOjOGF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhG1X0kVjOTz"
      },
      "source": [
        "# 先頭10行の確認\n",
        "!head -10 ./newsCorpora.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1_ifZLujQYa"
      },
      "source": [
        "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
        "!sed -e 's/\"/'\\''/g' ./newsCorpora.csv > ./newsCorpora_re.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3qXtCbzjS1f"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データの読込\n",
        "df = pd.read_csv('./newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "\n",
        "# データの抽出\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "# データの分割\n",
        "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "valid.reset_index(drop=True, inplace=True)\n",
        "test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(train.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1aLWu01jU-S"
      },
      "source": [
        "# 事例数の確認\n",
        "print('【学習データ】')\n",
        "print(train['CATEGORY'].value_counts())\n",
        "print('【検証データ】')\n",
        "print(valid['CATEGORY'].value_counts())\n",
        "print('【評価データ】')\n",
        "print(test['CATEGORY'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z59jjWrjXIe"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMoUuk9ajY-d"
      },
      "source": [
        "import numpy as np\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import optim\n",
        "from torch import cuda\n",
        "import time\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8PSComYjanf"
      },
      "source": [
        "# Datasetの定義\n",
        "class CreateDataset(Dataset):\n",
        "  def __init__(self, X, y, tokenizer, max_len):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):  # len(Dataset)で返す値を指定\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
        "    text = self.X[index]\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "      text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      pad_to_max_length=True\n",
        "    )\n",
        "    ids = inputs['input_ids']\n",
        "    mask = inputs['attention_mask']\n",
        "\n",
        "    return {\n",
        "      'ids': torch.LongTensor(ids),\n",
        "      'mask': torch.LongTensor(mask),\n",
        "      'labels': torch.Tensor(self.y[index])\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xesOT2S3jef9"
      },
      "source": [
        "# 正解ラベルのone-hot化\n",
        "y_train = pd.get_dummies(train, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
        "y_valid = pd.get_dummies(valid, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
        "y_test = pd.get_dummies(test, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
        "\n",
        "# Datasetの作成\n",
        "max_len = 20\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataset_train = CreateDataset(train['TITLE'], y_train, tokenizer, max_len)\n",
        "dataset_valid = CreateDataset(valid['TITLE'], y_valid, tokenizer, max_len)\n",
        "dataset_test = CreateDataset(test['TITLE'], y_test, tokenizer, max_len)\n",
        "\n",
        "for var in dataset_train[0]:\n",
        "  print(f'{var}: {dataset_train[0][var]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gKgdFR6jg34"
      },
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "  def __init__(self, drop_rate, otuput_size):\n",
        "    super().__init__()\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.drop = torch.nn.Dropout(drop_rate)\n",
        "    self.fc = torch.nn.Linear(768, otuput_size)  # BERTの出力に合わせて768次元を指定\n",
        "\n",
        "  def forward(self, ids, mask):\n",
        "    _, out = self.bert(ids, attention_mask=mask)\n",
        "    out = self.fc(self.drop(out))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgEEykiXjjRS"
      },
      "source": [
        "def calculate_loss_and_accuracy(model, criterion, loader, device):\n",
        "  \"\"\" 損失・正解率を計算\"\"\"\n",
        "  model.eval()\n",
        "  loss = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      # デバイスの指定\n",
        "      ids = data['ids'].to(device)\n",
        "      mask = data['mask'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "\n",
        "      # 順伝播\n",
        "      outputs = model(ids, mask)\n",
        "\n",
        "      # 損失計算\n",
        "      loss += criterion(outputs, labels).item()\n",
        "\n",
        "      # 正解率計算\n",
        "      pred = torch.argmax(outputs, dim=-1).cpu().numpy() # バッチサイズの長さの予測ラベル配列\n",
        "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # バッチサイズの長さの正解ラベル配列\n",
        "      total += len(labels)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return loss / len(loader), correct / total\n",
        "\n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
        "  \"\"\"モデルの学習を実行し、損失・正解率のログを返す\"\"\"\n",
        "  # デバイスの指定\n",
        "  model.to(device)\n",
        "\n",
        "  # dataloaderの作成\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "  # 学習\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(num_epochs):\n",
        "    # 開始時刻の記録\n",
        "    s_time = time.time()\n",
        "\n",
        "    # 訓練モードに設定\n",
        "    model.train()\n",
        "    for data in dataloader_train:\n",
        "      # デバイスの指定\n",
        "      ids = data['ids'].to(device)\n",
        "      mask = data['mask'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "\n",
        "      # 勾配をゼロで初期化\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "      outputs = model(ids, mask)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # 損失と正解率の算出\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train, device)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid, device)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    # チェックポイントの保存\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    # 終了時刻の記録\n",
        "    e_time = time.time()\n",
        "\n",
        "    # ログを出力\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G73IH3fPjmAh"
      },
      "source": [
        "# パラメータの設定\n",
        "DROP_RATE = 0.4\n",
        "OUTPUT_SIZE = 4\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# モデルの定義\n",
        "model = BERTClass(DROP_RATE, OUTPUT_SIZE)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# デバイスの指定\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeCS2iVsjs0w"
      },
      "source": [
        "# ログの可視化\n",
        "x_axis = [x for x in range(1, len(log['train']) + 1)]\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax[0].plot(x_axis, np.array(log['train']).T[0], label='train')\n",
        "ax[0].plot(x_axis, np.array(log['valid']).T[0], label='valid')\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].legend()\n",
        "ax[1].plot(x_axis, np.array(log['train']).T[1], label='train')\n",
        "ax[1].plot(x_axis, np.array(log['valid']).T[1], label='valid')\n",
        "ax[1].set_xlabel('epoch')\n",
        "ax[1].set_ylabel('accuracy')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYN_hIamjtSh"
      },
      "source": [
        "# 正解率の算出\n",
        "def calculate_accuracy(model, dataset, device):\n",
        "  # Dataloaderの作成\n",
        "  loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      # デバイスの指定\n",
        "      ids = data['ids'].to(device)\n",
        "      mask = data['mask'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "\n",
        "      # 順伝播 + 予測値の取得 + 正解数のカウント\n",
        "      outputs = model.forward(ids, mask)\n",
        "      pred = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
        "      labels = torch.argmax(labels, dim=-1).cpu().numpy()\n",
        "      total += len(labels)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return correct / total\n",
        "\n",
        "print(f'正解率（学習データ）：{calculate_accuracy(model, dataset_train, device):.3f}')\n",
        "print(f'正解率（検証データ）：{calculate_accuracy(model, dataset_valid, device):.3f}')\n",
        "print(f'正解率（評価データ）：{calculate_accuracy(model, dataset_test, device):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}